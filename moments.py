import io
import logging
import json
import os
import pickle
import time
from typing import List, Dict, Any

import numpy as np
import pandas as pd
import streamlit as st
from openai import OpenAI
from rapidfuzz import fuzz
from sklearn.cluster import AgglomerativeClustering
from semhash import SemHash
import spacy

# âœ… DODANE: normalizacja
import re
import unidecode

# -----------------------------
# Page Configuration
# -----------------------------
st.set_page_config(
    page_title="ğŸ” Groupowanie fraz â†’ Excel Brief Pipeline",
    initial_sidebar_state="expanded"
)

st.sidebar.header("âš™ï¸ Configuration")

# API Key
OPENAI_API_KEY = st.sidebar.text_input("OpenAI API Key", type="password")

# Models - wybÃ³r z listy
OPENAI_EMBEDDING_MODEL = st.sidebar.selectbox(
    "Model embeddingÃ³w",
    ["text-embedding-3-large", "text-embedding-3-small"],
    index=0
)

OPENAI_CHAT_MODEL = st.sidebar.selectbox(
    "Model czatu (dla briefÃ³w)",
    ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1"],
    index=0
)

# Parameters with explanations
DEDUP_THRESHOLD = st.sidebar.slider(
    "Deduplication Threshold (RapidFuzz)", 0, 100, 85, 1
)
CLUSTER_SIM = st.sidebar.slider(
    "Initial Clustering Similarity Threshold", 0.0, 1.0, 0.80, 0.01
)
MERGE_SIM = st.sidebar.slider(
    "Cluster Merge Similarity Threshold", 0.0, 1.0, 0.85, 0.01
)
SEMHASH_SIM = st.sidebar.slider(
    "SemHash Similarity Threshold", 0.80, 0.99, 0.95, 0.01
)
USE_SEMHASH = st.sidebar.checkbox("UÅ¼yj SemHash do deduplikacji", value=False)

# -----------------------------
# NOWA FUNKCJA: WYCZYSZCZENIE CHECKPOINTA
# -----------------------------
if st.sidebar.button("ğŸ—‘ï¸ WyczyÅ›Ä‡ checkpoint"):
    if os.path.exists("briefs.pkl"):
        os.remove("briefs.pkl")
        st.sidebar.success("Checkpoint zostaÅ‚ usuniÄ™ty! ğŸ”¥")
    else:
        st.sidebar.info("Brak pliku checkpointa do usuniÄ™cia.")

# -----------------------------
# Parametry â€“ objaÅ›nienia
# -----------------------------
st.sidebar.markdown("### â„¹ï¸ ObjaÅ›nienia parametrÃ³w")
st.sidebar.info("""
**Deduplication Threshold (RapidFuzz)** â€“ prÃ³g podobieÅ„stwa (0â€“100), powyÅ¼ej ktÃ³rego frazy sÄ… traktowane jako duplikaty.  
**Initial Clustering Similarity Threshold** â€“ minimalne podobieÅ„stwo (0â€“1), Å¼eby frazy trafiÅ‚y do tego samego klastra.  
**Cluster Merge Similarity Threshold** â€“ prÃ³g podobieÅ„stwa (0â€“1), przy ktÃ³rym Å‚Ä…czymy klastry.  
**SemHash Similarity Threshold** â€“ jeÅ›li uÅ¼yjesz SemHash, okreÅ›la jak bliskie semantycznie muszÄ… byÄ‡ frazy, Å¼eby uznaÄ‡ je za duplikaty.
""")

# -----------------------------
# NLP â€“ Lematyzacja (spaCy)
# -----------------------------
@st.cache_resource
def load_spacy():
    try:
        return spacy.load("pl_core_news_sm")
    except:
        st.warning("âš ï¸ Musisz zainstalowaÄ‡ model spaCy: python -m spacy download pl_core_news_sm")
        return None

nlp = load_spacy()

def lemmatize_texts(texts: List[str]) -> List[str]:
    """Zwraca lematy tekstÃ³w (uÅ¼ywane tylko do embeddingÃ³w)."""
    if not nlp:
        return texts
    return [" ".join([token.lemma_.lower() for token in nlp(t)]) for t in texts]

# -----------------------------
# âœ… DODANE: normalizacja fraz
# -----------------------------
def normalize_phrase(s: str) -> str:
    s = str(s).strip().lower()
    s = unidecode.unidecode(s)
    s = re.sub(r"[^a-z0-9\s]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

# -----------------------------
# Helpers
# -----------------------------
# âœ… ZMIENIONE: dedup na token_set_ratio + normalizacja
def deduplicate(questions: List[str], threshold: int = 85) -> List[str]:
    unique = []
    unique_norm = []
    for q in questions:
        nq = normalize_phrase(q)
        if not any(fuzz.token_set_ratio(nq, u) >= threshold for u in unique_norm):
            unique.append(q)
            unique_norm.append(nq)
    return unique

def semhash_deduplicate(questions: List[str], threshold: float = 0.95) -> List[str]:
    try:
        sh = SemHash.from_records(records=questions)
        result = sh.self_deduplicate(threshold=threshold)
        if hasattr(result, "selected"):
            return result.selected
        elif hasattr(result, "deduplicated"):
            return result.deduplicated
        elif isinstance(result, list):
            return result
        else:
            return deduplicate(questions, threshold=90)
    except Exception as e:
        logging.warning(f"âš ï¸ SemHash failed ({e}) â†’ fallback RapidFuzz")
        return deduplicate(questions, threshold=90)

def embed_texts(client: OpenAI, texts: List[str], model=OPENAI_EMBEDDING_MODEL) -> np.ndarray:
    response = client.embeddings.create(model=model, input=texts)
    return np.array([d.embedding for d in response.data])

def cluster_questions(questions: List[str], embeddings: np.ndarray, sim_threshold=0.8) -> Dict[int, List[str]]:
    if not questions:
        return {}
    clustering = AgglomerativeClustering(
        n_clusters=None,
        metric="cosine",
        linkage="average",
        distance_threshold=1 - sim_threshold,
    )
    labels = clustering.fit_predict(embeddings)
    clustered: Dict[int, List[str]] = {}
    for label, q in zip(labels, questions):
        clustered.setdefault(int(label), []).append(q)
    return clustered

def merge_similar_clusters(clusters: Dict[int, List[str]], embeddings: np.ndarray, sim_threshold=0.85, q2i: Dict[str, int] = None) -> Dict[int, List[str]]:
    if not clusters:
        return {}
    centroids = {}
    for cid, qs in clusters.items():
        idxs = [q2i[q] for q in qs if q in q2i]
        if not idxs:
            continue
        centroid = np.mean(embeddings[idxs], axis=0)
        centroid = centroid / (np.linalg.norm(centroid) + 1e-12)
        centroids[cid] = centroid

    merged: Dict[int, List[str]] = {}
    used = set()
    new_id = 0
    cluster_ids = list(clusters.keys())

    for cid in cluster_ids:
        if cid in used or cid not in centroids:
            continue
        merged[new_id] = list(clusters[cid])
        used.add(cid)
        for cid2 in cluster_ids:
            if cid2 in used or cid2 not in centroids:
                continue
            sim = float(np.dot(centroids[cid], centroids[cid2]))
            if sim >= sim_threshold:
                merged[new_id].extend(clusters[cid2])
                used.add(cid2)
        new_id += 1
    return merged

# âœ… DODANE: dedup wewnÄ…trz klastrÃ³w po merge
def deduplicate_within_clusters(clusters: Dict[int, List[str]], threshold: int = 92) -> Dict[int, List[str]]:
    out: Dict[int, List[str]] = {}
    for cid, qs in clusters.items():
        qs = list(dict.fromkeys(qs))  # usuÅ„ identyczne
        out[cid] = deduplicate(qs, threshold=threshold)  # usuÅ„ semantyczne
    return out

# âœ… ZMIENIONE: global dedup na token_set_ratio + normalizacja
def global_deduplicate_clusters(clusters: Dict[int, List[str]], threshold: int = 90) -> Dict[int, List[str]]:
    seen_norm = []
    new_clusters: Dict[int, List[str]] = {}
    for cid, qs in clusters.items():
        unique_qs = []
        for q in qs:
            nq = normalize_phrase(q)
            if not any(fuzz.token_set_ratio(nq, s) >= threshold for s in seen_norm):
                unique_qs.append(q)
                seen_norm.append(nq)
        if unique_qs:
            new_clusters[cid] = unique_qs
    return new_clusters

# âœ… DODANE: stabilny wybÃ³r main_phrase
def pick_main_phrase(qs: List[str]) -> str:
    return sorted(qs, key=lambda x: (len(normalize_phrase(x)), len(str(x))))[0] if qs else ""

def generate_article_brief(questions: List[str], client: OpenAI | None, model: str = "gpt-4o-mini") -> Dict[str, Any]:
    if client is None:
        return {"intencja": "", "frazy": ", ".join(questions), "tytul": "", "wytyczne": ""}
    prompt = f"""
Dla poniÅ¼szej listy fraz przygotuj dane do planu artykuÅ‚u.

Frazy: {questions}

Odpowiedz w formacie:

Intencja: [typ intencji wyszukiwania]
Frazy: [lista fraz long-tail, rozdzielona przecinkami]
TytuÅ‚: [SEO-friendly, max 70 znakÃ³w, naturalny, z gÅ‚Ã³wnym keywordem]
Wytyczne: [2â€“3 zdania opisu oczekiwaÅ„ uÅ¼ytkownika]
"""
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "JesteÅ› asystentem SEO. Zawsze trzymaj siÄ™ formatu."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.7,
        )
        content = resp.choices[0].message.content.strip()
        result = {"intencja": "", "frazy": "", "tytul": "", "wytyczne": ""}
        for line in content.splitlines():
            low = line.lower()
            if low.startswith("intencja:"):
                result["intencja"] = line.split(":", 1)[1].strip()
            elif low.startswith("frazy:"):
                result["frazy"] = line.split(":", 1)[1].strip()
            elif low.startswith("tytuÅ‚:") or low.startswith("tytul:"):
                result["tytul"] = line.split(":", 1)[1].strip()
            elif low.startswith("wytyczne:"):
                result["wytyczne"] = line.split(":", 1)[1].strip()
        result["frazy"] = result["frazy"] or ", ".join(questions)
        return result
    except Exception as e:
        logging.warning(f"âš ï¸ Brief parse failed: {e}")
        return {"intencja": "", "frazy": ", ".join(questions), "tytul": "", "wytyczne": ""}

# -----------------------------
# Main App
# -----------------------------
st.title("ğŸ” Groupowanie fraz â†’ Excel Brief Pipeline")

status = st.empty()
progress_bar = st.progress(0)
log_box = st.container()

def update_status(message: str, progress: int):
    status.text(message)
    progress_bar.progress(progress)
    log_box.write(message)

phrases_input = st.sidebar.text_area("Wklej frazy, jedna na liniÄ™:")

# -----------------------------
# Checkpoint helpers
# -----------------------------
def save_checkpoint(data, filename="briefs.pkl"):
    with open(filename, "wb") as f:
        pickle.dump(data, f)

def load_checkpoint(filename="briefs.pkl"):
    if os.path.exists(filename):
        with open(filename, "rb") as f:
            return pickle.load(f)
    return []

# -----------------------------
# GÅ‚Ã³wna logika aplikacji
# -----------------------------
if st.sidebar.button("Uruchom grupowanie"):
    if not phrases_input.strip():
        st.warning("âš ï¸ Wklej najpierw listÄ™ fraz.")
        st.stop()

    if not OPENAI_API_KEY:
        st.error("âš ï¸ Podaj OpenAI API Key w panelu bocznym.")
        st.stop()

    openai_client = OpenAI(api_key=OPENAI_API_KEY)

    questions = [line.strip() for line in phrases_input.splitlines() if line.strip()]
    update_status(f"ğŸ“¥ Wczytano frazy: {len(questions)}", 5)

    if USE_SEMHASH:
        filtered = semhash_deduplicate(questions, threshold=SEMHASH_SIM)
        update_status(f"ğŸ§¹ Deduplication (SemHash {SEMHASH_SIM}): {len(questions)} â†’ {len(filtered)}", 15)
    else:
        filtered = deduplicate(questions, threshold=DEDUP_THRESHOLD)
        update_status(f"ğŸ§¹ Deduplication (RapidFuzz {DEDUP_THRESHOLD}): {len(questions)} â†’ {len(filtered)}", 15)

    update_status("ğŸ§  Generowanie embeddingÃ³w...", 35)
    lemmatized = lemmatize_texts(filtered)
    embeddings = embed_texts(openai_client, lemmatized, model=OPENAI_EMBEDDING_MODEL)
    q2i = {q: i for i, q in enumerate(filtered)}

    clusters = cluster_questions(filtered, embeddings, sim_threshold=CLUSTER_SIM)
    update_status(f"ğŸ§© Klastrowanie fraz: powstaÅ‚o {len(clusters)} klastrÃ³w", 55)

    clusters = merge_similar_clusters(clusters, embeddings, sim_threshold=MERGE_SIM, q2i=q2i)
    update_status(f"ğŸ”— Scalanie podobnych klastrÃ³w (prÃ³g {MERGE_SIM}): teraz {len(clusters)} klastrÃ³w", 70)

    # âœ… DODANE: dedup wewnÄ…trz klastrÃ³w po merge (Å¼eby nie wracaÅ‚y powtÃ³rki)
    clusters = deduplicate_within_clusters(clusters, threshold=92)

    clusters = global_deduplicate_clusters(clusters, threshold=90)
    update_status(f"ğŸ§½ Usuwanie duplikatÃ³w miÄ™dzy klastrami: {len(clusters)} koÅ„cowych klastrÃ³w", 85)

    update_status(f"âœ… PominiÄ™to walidacjÄ™ LLM â€“ pozostawiono {len(clusters)} klastrÃ³w po klasycznym scaleniu", 90)

    # -----------------------------
    # Wczytaj checkpoint i generuj briefy
    # -----------------------------
    rows = load_checkpoint()
    done = len(rows)
    total = len(clusters)

    if done > 0:
        update_status(f"ğŸ” Wczytano {done} gotowych briefÃ³w z checkpointa", 90)
    else:
        update_status("ğŸ“ Rozpoczynam generowanie briefÃ³w od poczÄ…tku", 90)

    for i, (label, qs) in enumerate(clusters.items(), 1):
        if i <= done:
            continue
        try:
            update_status(f"ğŸ“ GenerujÄ™ brief {i}/{total} ({len(qs)} fraz)", int(95 * i / total))
            brief = generate_article_brief(qs, openai_client, model=OPENAI_CHAT_MODEL)
            rows.append({
                "cluster_id": label,
                "main_phrase": pick_main_phrase(qs),  # âœ… ZMIENIONE: stabilny wybÃ³r
                "intencja": brief.get("intencja", ""),
                "frazy": ", ".join(qs),
                "tytul": brief.get("tytul", ""),
                "wytyczne": brief.get("wytyczne", ""),
            })
            save_checkpoint(rows)
            time.sleep(1.5)
        except Exception as e:
            logging.warning(f"âš ï¸ BÅ‚Ä…d przy klastrze {i}/{total}: {e}")
            time.sleep(3)
            continue

    update_status("âœ… Wszystkie briefy wygenerowane lub wczytane z checkpointa", 98)

    # -----------------------------
    # Zapis do Excela
    # -----------------------------
    df = pd.DataFrame(rows)
    xlsx_buffer = io.BytesIO()
    with pd.ExcelWriter(xlsx_buffer, engine="openpyxl") as writer:
        df.to_excel(writer, sheet_name="Briefs", index=False)
    xlsx_buffer.seek(0)

    st.session_state["excel_buffer"] = xlsx_buffer.getvalue()
    st.session_state["results"] = rows

    update_status("âœ… Gotowe!", 100)

if "excel_buffer" in st.session_state:
    st.download_button(
        label="ğŸ“¥ Pobierz Excel",
        data=st.session_state["excel_buffer"],
        file_name="frazy_briefy.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )
    st.success("âœ… ZakoÅ„czono przetwarzanie.")
    st.subheader("ğŸ“Š PodglÄ…d wynikÃ³w")
    st.dataframe(pd.DataFrame(st.session_state["results"]))




